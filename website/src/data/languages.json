[
  {
    "code": "en",
    "name": "English",
    "common_name": "English",
    "alpha_2": "en",
    "alpha_3": "eng",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 1250000,
    "best_compression_ratio": 4.2,
    "best_isotropy": 0.89,
    "has_models": true,
    "model_card_excerpt": "English language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/en",
    "visualizations_base": "https://huggingface.co/wikilangs/en/resolve/main/visualizations"
  },
  {
    "code": "fr",
    "name": "French",
    "common_name": "French",
    "alpha_2": "fr",
    "alpha_3": "fra",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 890000,
    "best_compression_ratio": 4.1,
    "best_isotropy": 0.87,
    "has_models": true,
    "model_card_excerpt": "French language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/fr",
    "visualizations_base": "https://huggingface.co/wikilangs/fr/resolve/main/visualizations"
  },
  {
    "code": "de",
    "name": "German",
    "common_name": "German",
    "alpha_2": "de",
    "alpha_3": "deu",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 1100000,
    "best_compression_ratio": 3.9,
    "best_isotropy": 0.86,
    "has_models": true,
    "model_card_excerpt": "German language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/de",
    "visualizations_base": "https://huggingface.co/wikilangs/de/resolve/main/visualizations"
  },
  {
    "code": "es",
    "name": "Spanish",
    "common_name": "Spanish",
    "alpha_2": "es",
    "alpha_3": "spa",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 920000,
    "best_compression_ratio": 4.3,
    "best_isotropy": 0.88,
    "has_models": true,
    "model_card_excerpt": "Spanish language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/es",
    "visualizations_base": "https://huggingface.co/wikilangs/es/resolve/main/visualizations"
  },
  {
    "code": "zh",
    "name": "Chinese",
    "common_name": "Chinese",
    "alpha_2": "zh",
    "alpha_3": "zho",
    "scope": "macrolanguage",
    "language_type": "living",
    "vocabulary_size": 680000,
    "best_compression_ratio": 2.8,
    "best_isotropy": 0.82,
    "has_models": true,
    "model_card_excerpt": "Chinese language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/zh",
    "visualizations_base": "https://huggingface.co/wikilangs/zh/resolve/main/visualizations"
  },
  {
    "code": "ja",
    "name": "Japanese",
    "common_name": "Japanese",
    "alpha_2": "ja",
    "alpha_3": "jpn",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 520000,
    "best_compression_ratio": 2.6,
    "best_isotropy": 0.79,
    "has_models": true,
    "model_card_excerpt": "Japanese language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/ja",
    "visualizations_base": "https://huggingface.co/wikilangs/ja/resolve/main/visualizations"
  },
  {
    "code": "ar",
    "name": "Arabic",
    "common_name": "Arabic",
    "alpha_2": "ar",
    "alpha_3": "ara",
    "scope": "macrolanguage",
    "language_type": "living",
    "vocabulary_size": 480000,
    "best_compression_ratio": 3.5,
    "best_isotropy": 0.84,
    "has_models": true,
    "model_card_excerpt": "Arabic language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/ar",
    "visualizations_base": "https://huggingface.co/wikilangs/ar/resolve/main/visualizations"
  },
  {
    "code": "ru",
    "name": "Russian",
    "common_name": "Russian",
    "alpha_2": "ru",
    "alpha_3": "rus",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 780000,
    "best_compression_ratio": 3.7,
    "best_isotropy": 0.85,
    "has_models": true,
    "model_card_excerpt": "Russian language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/ru",
    "visualizations_base": "https://huggingface.co/wikilangs/ru/resolve/main/visualizations"
  },
  {
    "code": "pt",
    "name": "Portuguese",
    "common_name": "Portuguese",
    "alpha_2": "pt",
    "alpha_3": "por",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 620000,
    "best_compression_ratio": 4.1,
    "best_isotropy": 0.86,
    "has_models": true,
    "model_card_excerpt": "Portuguese language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/pt",
    "visualizations_base": "https://huggingface.co/wikilangs/pt/resolve/main/visualizations"
  },
  {
    "code": "hi",
    "name": "Hindi",
    "common_name": "Hindi",
    "alpha_2": "hi",
    "alpha_3": "hin",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 280000,
    "best_compression_ratio": 3.2,
    "best_isotropy": 0.81,
    "has_models": true,
    "model_card_excerpt": "Hindi language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/hi",
    "visualizations_base": "https://huggingface.co/wikilangs/hi/resolve/main/visualizations"
  },
  {
    "code": "ko",
    "name": "Korean",
    "common_name": "Korean",
    "alpha_2": "ko",
    "alpha_3": "kor",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 350000,
    "best_compression_ratio": 2.9,
    "best_isotropy": 0.80,
    "has_models": true,
    "model_card_excerpt": "Korean language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/ko",
    "visualizations_base": "https://huggingface.co/wikilangs/ko/resolve/main/visualizations"
  },
  {
    "code": "ary",
    "name": "Moroccan Arabic",
    "common_name": "Darija",
    "alpha_2": null,
    "alpha_3": "ary",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 45000,
    "best_compression_ratio": 3.4,
    "best_isotropy": 0.78,
    "has_models": true,
    "model_card_excerpt": "Moroccan Arabic (Darija) language models trained on Wikipedia. A low-resource language with rich cultural significance.",
    "hf_url": "https://huggingface.co/wikilangs/ary",
    "visualizations_base": "https://huggingface.co/wikilangs/ary/resolve/main/visualizations"
  },
  {
    "code": "sw",
    "name": "Swahili",
    "common_name": "Swahili",
    "alpha_2": "sw",
    "alpha_3": "swa",
    "scope": "macrolanguage",
    "language_type": "living",
    "vocabulary_size": 120000,
    "best_compression_ratio": 4.0,
    "best_isotropy": 0.83,
    "has_models": true,
    "model_card_excerpt": "Swahili language models trained on Wikipedia. A major African language spoken across East Africa.",
    "hf_url": "https://huggingface.co/wikilangs/sw",
    "visualizations_base": "https://huggingface.co/wikilangs/sw/resolve/main/visualizations"
  },
  {
    "code": "uk",
    "name": "Ukrainian",
    "common_name": "Ukrainian",
    "alpha_2": "uk",
    "alpha_3": "ukr",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 520000,
    "best_compression_ratio": 3.6,
    "best_isotropy": 0.84,
    "has_models": true,
    "model_card_excerpt": "Ukrainian language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/uk",
    "visualizations_base": "https://huggingface.co/wikilangs/uk/resolve/main/visualizations"
  },
  {
    "code": "vi",
    "name": "Vietnamese",
    "common_name": "Vietnamese",
    "alpha_2": "vi",
    "alpha_3": "vie",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 380000,
    "best_compression_ratio": 3.8,
    "best_isotropy": 0.82,
    "has_models": true,
    "model_card_excerpt": "Vietnamese language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/vi",
    "visualizations_base": "https://huggingface.co/wikilangs/vi/resolve/main/visualizations"
  },
  {
    "code": "fa",
    "name": "Persian",
    "common_name": "Persian",
    "alpha_2": "fa",
    "alpha_3": "fas",
    "scope": "macrolanguage",
    "language_type": "living",
    "vocabulary_size": 420000,
    "best_compression_ratio": 3.4,
    "best_isotropy": 0.83,
    "has_models": true,
    "model_card_excerpt": "Persian language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/fa",
    "visualizations_base": "https://huggingface.co/wikilangs/fa/resolve/main/visualizations"
  },
  {
    "code": "it",
    "name": "Italian",
    "common_name": "Italian",
    "alpha_2": "it",
    "alpha_3": "ita",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 680000,
    "best_compression_ratio": 4.0,
    "best_isotropy": 0.87,
    "has_models": true,
    "model_card_excerpt": "Italian language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/it",
    "visualizations_base": "https://huggingface.co/wikilangs/it/resolve/main/visualizations"
  },
  {
    "code": "nl",
    "name": "Dutch",
    "common_name": "Dutch",
    "alpha_2": "nl",
    "alpha_3": "nld",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 580000,
    "best_compression_ratio": 3.8,
    "best_isotropy": 0.85,
    "has_models": true,
    "model_card_excerpt": "Dutch language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/nl",
    "visualizations_base": "https://huggingface.co/wikilangs/nl/resolve/main/visualizations"
  },
  {
    "code": "pl",
    "name": "Polish",
    "common_name": "Polish",
    "alpha_2": "pl",
    "alpha_3": "pol",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 620000,
    "best_compression_ratio": 3.5,
    "best_isotropy": 0.84,
    "has_models": true,
    "model_card_excerpt": "Polish language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/pl",
    "visualizations_base": "https://huggingface.co/wikilangs/pl/resolve/main/visualizations"
  },
  {
    "code": "tr",
    "name": "Turkish",
    "common_name": "Turkish",
    "alpha_2": "tr",
    "alpha_3": "tur",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 420000,
    "best_compression_ratio": 3.3,
    "best_isotropy": 0.82,
    "has_models": true,
    "model_card_excerpt": "Turkish language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/tr",
    "visualizations_base": "https://huggingface.co/wikilangs/tr/resolve/main/visualizations"
  },
  {
    "code": "th",
    "name": "Thai",
    "common_name": "Thai",
    "alpha_2": "th",
    "alpha_3": "tha",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 180000,
    "best_compression_ratio": 2.7,
    "best_isotropy": 0.78,
    "has_models": true,
    "model_card_excerpt": "Thai language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/th",
    "visualizations_base": "https://huggingface.co/wikilangs/th/resolve/main/visualizations"
  },
  {
    "code": "he",
    "name": "Hebrew",
    "common_name": "Hebrew",
    "alpha_2": "he",
    "alpha_3": "heb",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 280000,
    "best_compression_ratio": 3.1,
    "best_isotropy": 0.81,
    "has_models": true,
    "model_card_excerpt": "Hebrew language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/he",
    "visualizations_base": "https://huggingface.co/wikilangs/he/resolve/main/visualizations"
  },
  {
    "code": "el",
    "name": "Greek",
    "common_name": "Greek",
    "alpha_2": "el",
    "alpha_3": "ell",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 320000,
    "best_compression_ratio": 3.4,
    "best_isotropy": 0.83,
    "has_models": true,
    "model_card_excerpt": "Greek language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/el",
    "visualizations_base": "https://huggingface.co/wikilangs/el/resolve/main/visualizations"
  },
  {
    "code": "cs",
    "name": "Czech",
    "common_name": "Czech",
    "alpha_2": "cs",
    "alpha_3": "ces",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 420000,
    "best_compression_ratio": 3.5,
    "best_isotropy": 0.84,
    "has_models": true,
    "model_card_excerpt": "Czech language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/cs",
    "visualizations_base": "https://huggingface.co/wikilangs/cs/resolve/main/visualizations"
  },
  {
    "code": "sv",
    "name": "Swedish",
    "common_name": "Swedish",
    "alpha_2": "sv",
    "alpha_3": "swe",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 480000,
    "best_compression_ratio": 3.7,
    "best_isotropy": 0.85,
    "has_models": true,
    "model_card_excerpt": "Swedish language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/sv",
    "visualizations_base": "https://huggingface.co/wikilangs/sv/resolve/main/visualizations"
  },
  {
    "code": "hu",
    "name": "Hungarian",
    "common_name": "Hungarian",
    "alpha_2": "hu",
    "alpha_3": "hun",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 380000,
    "best_compression_ratio": 3.2,
    "best_isotropy": 0.82,
    "has_models": true,
    "model_card_excerpt": "Hungarian language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/hu",
    "visualizations_base": "https://huggingface.co/wikilangs/hu/resolve/main/visualizations"
  },
  {
    "code": "fi",
    "name": "Finnish",
    "common_name": "Finnish",
    "alpha_2": "fi",
    "alpha_3": "fin",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 380000,
    "best_compression_ratio": 3.0,
    "best_isotropy": 0.81,
    "has_models": true,
    "model_card_excerpt": "Finnish language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/fi",
    "visualizations_base": "https://huggingface.co/wikilangs/fi/resolve/main/visualizations"
  },
  {
    "code": "da",
    "name": "Danish",
    "common_name": "Danish",
    "alpha_2": "da",
    "alpha_3": "dan",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 280000,
    "best_compression_ratio": 3.8,
    "best_isotropy": 0.84,
    "has_models": true,
    "model_card_excerpt": "Danish language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/da",
    "visualizations_base": "https://huggingface.co/wikilangs/da/resolve/main/visualizations"
  },
  {
    "code": "no",
    "name": "Norwegian",
    "common_name": "Norwegian",
    "alpha_2": "no",
    "alpha_3": "nor",
    "scope": "macrolanguage",
    "language_type": "living",
    "vocabulary_size": 320000,
    "best_compression_ratio": 3.7,
    "best_isotropy": 0.84,
    "has_models": true,
    "model_card_excerpt": "Norwegian language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/no",
    "visualizations_base": "https://huggingface.co/wikilangs/no/resolve/main/visualizations"
  },
  {
    "code": "ro",
    "name": "Romanian",
    "common_name": "Romanian",
    "alpha_2": "ro",
    "alpha_3": "ron",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 320000,
    "best_compression_ratio": 3.9,
    "best_isotropy": 0.85,
    "has_models": true,
    "model_card_excerpt": "Romanian language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/ro",
    "visualizations_base": "https://huggingface.co/wikilangs/ro/resolve/main/visualizations"
  },
  {
    "code": "id",
    "name": "Indonesian",
    "common_name": "Indonesian",
    "alpha_2": "id",
    "alpha_3": "ind",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 280000,
    "best_compression_ratio": 4.1,
    "best_isotropy": 0.86,
    "has_models": true,
    "model_card_excerpt": "Indonesian language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/id",
    "visualizations_base": "https://huggingface.co/wikilangs/id/resolve/main/visualizations"
  },
  {
    "code": "bn",
    "name": "Bengali",
    "common_name": "Bengali",
    "alpha_2": "bn",
    "alpha_3": "ben",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 180000,
    "best_compression_ratio": 3.0,
    "best_isotropy": 0.79,
    "has_models": true,
    "model_card_excerpt": "Bengali language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/bn",
    "visualizations_base": "https://huggingface.co/wikilangs/bn/resolve/main/visualizations"
  },
  {
    "code": "ta",
    "name": "Tamil",
    "common_name": "Tamil",
    "alpha_2": "ta",
    "alpha_3": "tam",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 150000,
    "best_compression_ratio": 2.9,
    "best_isotropy": 0.78,
    "has_models": true,
    "model_card_excerpt": "Tamil language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/ta",
    "visualizations_base": "https://huggingface.co/wikilangs/ta/resolve/main/visualizations"
  },
  {
    "code": "te",
    "name": "Telugu",
    "common_name": "Telugu",
    "alpha_2": "te",
    "alpha_3": "tel",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 120000,
    "best_compression_ratio": 2.8,
    "best_isotropy": 0.77,
    "has_models": true,
    "model_card_excerpt": "Telugu language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/te",
    "visualizations_base": "https://huggingface.co/wikilangs/te/resolve/main/visualizations"
  },
  {
    "code": "mr",
    "name": "Marathi",
    "common_name": "Marathi",
    "alpha_2": "mr",
    "alpha_3": "mar",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 95000,
    "best_compression_ratio": 2.9,
    "best_isotropy": 0.78,
    "has_models": true,
    "model_card_excerpt": "Marathi language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/mr",
    "visualizations_base": "https://huggingface.co/wikilangs/mr/resolve/main/visualizations"
  },
  {
    "code": "ur",
    "name": "Urdu",
    "common_name": "Urdu",
    "alpha_2": "ur",
    "alpha_3": "urd",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 110000,
    "best_compression_ratio": 3.1,
    "best_isotropy": 0.80,
    "has_models": true,
    "model_card_excerpt": "Urdu language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/ur",
    "visualizations_base": "https://huggingface.co/wikilangs/ur/resolve/main/visualizations"
  },
  {
    "code": "ms",
    "name": "Malay",
    "common_name": "Malay",
    "alpha_2": "ms",
    "alpha_3": "msa",
    "scope": "macrolanguage",
    "language_type": "living",
    "vocabulary_size": 180000,
    "best_compression_ratio": 4.0,
    "best_isotropy": 0.85,
    "has_models": true,
    "model_card_excerpt": "Malay language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/ms",
    "visualizations_base": "https://huggingface.co/wikilangs/ms/resolve/main/visualizations"
  },
  {
    "code": "tl",
    "name": "Tagalog",
    "common_name": "Tagalog",
    "alpha_2": "tl",
    "alpha_3": "tgl",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 95000,
    "best_compression_ratio": 3.9,
    "best_isotropy": 0.84,
    "has_models": true,
    "model_card_excerpt": "Tagalog language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/tl",
    "visualizations_base": "https://huggingface.co/wikilangs/tl/resolve/main/visualizations"
  },
  {
    "code": "ca",
    "name": "Catalan",
    "common_name": "Catalan",
    "alpha_2": "ca",
    "alpha_3": "cat",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 320000,
    "best_compression_ratio": 4.0,
    "best_isotropy": 0.86,
    "has_models": true,
    "model_card_excerpt": "Catalan language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/ca",
    "visualizations_base": "https://huggingface.co/wikilangs/ca/resolve/main/visualizations"
  },
  {
    "code": "eu",
    "name": "Basque",
    "common_name": "Basque",
    "alpha_2": "eu",
    "alpha_3": "eus",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 180000,
    "best_compression_ratio": 3.2,
    "best_isotropy": 0.81,
    "has_models": true,
    "model_card_excerpt": "Basque language models trained on Wikipedia. A language isolate with unique linguistic properties.",
    "hf_url": "https://huggingface.co/wikilangs/eu",
    "visualizations_base": "https://huggingface.co/wikilangs/eu/resolve/main/visualizations"
  },
  {
    "code": "gl",
    "name": "Galician",
    "common_name": "Galician",
    "alpha_2": "gl",
    "alpha_3": "glg",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 180000,
    "best_compression_ratio": 4.1,
    "best_isotropy": 0.86,
    "has_models": true,
    "model_card_excerpt": "Galician language models trained on Wikipedia. Includes BPE tokenizers, n-gram models, Markov chains, and word embeddings.",
    "hf_url": "https://huggingface.co/wikilangs/gl",
    "visualizations_base": "https://huggingface.co/wikilangs/gl/resolve/main/visualizations"
  },
  {
    "code": "cy",
    "name": "Welsh",
    "common_name": "Welsh",
    "alpha_2": "cy",
    "alpha_3": "cym",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 95000,
    "best_compression_ratio": 3.5,
    "best_isotropy": 0.82,
    "has_models": true,
    "model_card_excerpt": "Welsh language models trained on Wikipedia. A Celtic language with revitalization efforts.",
    "hf_url": "https://huggingface.co/wikilangs/cy",
    "visualizations_base": "https://huggingface.co/wikilangs/cy/resolve/main/visualizations"
  },
  {
    "code": "ga",
    "name": "Irish",
    "common_name": "Irish",
    "alpha_2": "ga",
    "alpha_3": "gle",
    "scope": "individual",
    "language_type": "living",
    "vocabulary_size": 65000,
    "best_compression_ratio": 3.4,
    "best_isotropy": 0.81,
    "has_models": true,
    "model_card_excerpt": "Irish language models trained on Wikipedia. A Celtic language with official EU status.",
    "hf_url": "https://huggingface.co/wikilangs/ga",
    "visualizations_base": "https://huggingface.co/wikilangs/ga/resolve/main/visualizations"
  }
]
