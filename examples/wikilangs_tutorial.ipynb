{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikilangs Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the wikilangs package to work with pre-trained language models from Wikipedia data.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "1. Tokenizers (BPE)\n",
    "2. N-gram Models\n",
    "3. Markov Chains\n",
    "4. Vocabularies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package (if not already installed)\n",
    "# !pip install wikilangs\n",
    "\n",
    "# Import the modules\n",
    "from wikilangs import tokenizer, ngram, markov, vocabulary\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizers\n",
    "\n",
    "BPE tokenizers trained on Wikipedia data for different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer for English\n",
    "tok = tokenizer(date='20251201', lang='en', vocab_size=16000)\n",
    "\n",
    "# Tokenize some text\n",
    "text = \"This is a sample sentence for tokenization.\"\n",
    "tokens = tok.tokenize(text)\n",
    "token_ids = tok.encode(text)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Decoded text: {tok.decode(token_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-gram Models\n",
    "\n",
    "N-gram language models for text scoring and next token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3-gram model for English\n",
    "ng = ngram(date='20251201', lang='en', gram_size=3)\n",
    "\n",
    "# Score a text\n",
    "text = \"This is a sample sentence.\"\n",
    "score = ng.score(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Score: {score}\")\n",
    "\n",
    "# Predict next token\n",
    "context = \"This is a\"\n",
    "predictions = ng.predict_next(context, top_k=5)\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Markov Chains\n",
    "\n",
    "Markov chain models for text generation with configurable depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Markov chain for English\n",
    "mc = markov(date='20251201', lang='en', depth=2)\n",
    "\n",
    "# Generate text\n",
    "generated_text = mc.generate(length=50)\n",
    "\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding models\n",
    "\n",
    "Position-aware word embedding models with dimensions 32, 64, 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikilangs import tokenizer, embeddings\n",
    "\n",
    "# Date defaults to 'latest'\n",
    "tok = tokenizer(lang='ary')\n",
    "emb = embeddings(lang='ary')\n",
    "\n",
    "print(tok.tokenize(\"مرحبا\"))\n",
    "print(emb.embed_sentence(\"مرحبا بالعالم\", method='rope'))\n",
    "\n",
    "# Defaults to 32 but you can set a higher dimension\n",
    "emb64 = embeddings(lang='ary', dimension=64)\n",
    "print(emb64.embed_sentence(\"مرحبا بالعالم\", method='rope'))\n",
    "\n",
    "emb128 = embeddings(lang='ary', dimension=128)\n",
    "print(emb128.embed_sentence(\"مرحبا بالعالم\", method='rope'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vocabularies\n",
    "\n",
    "Comprehensive word dictionaries with frequency information using [vocabulous](https://github.com/omarkamali/vocabulous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary for English\n",
    "vocab = vocabulary(date='20251201', lang='en')\n",
    "\n",
    "# Look up a word\n",
    "word = \"example\"\n",
    "word_info = vocab.lookup(word)\n",
    "frequency = vocab.get_frequency(word)\n",
    "\n",
    "print(f\"Word: {word}\")\n",
    "print(f\"Information: {word_info}\")\n",
    "print(f\"Frequency: {frequency}\")\n",
    "\n",
    "# Get similar words\n",
    "similar = vocab.get_similar_words(word, top_k=5)\n",
    "print(f\"Similar words: {similar}\")\n",
    "\n",
    "# Get words with prefix\n",
    "prefixed = vocab.get_words_with_prefix(\"ex\", top_k=5)\n",
    "print(f\"Words with prefix 'ex': {prefixed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Languages\n",
    "\n",
    "The wikilangs package supports 100+ Wikipedia languages. Here's an example with French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for French\n",
    "try:\n",
    "    fr_tok = tokenizer(date='20251201', lang='fr', vocab_size=16000)\n",
    "    fr_ng = ngram(date='20251201', lang='fr', gram_size=3)\n",
    "    fr_mc = markov(date='20251201', lang='fr', depth=2)\n",
    "    fr_vocab = vocabulary(date='20251201', lang='fr')\n",
    "    \n",
    "    print(\"French models loaded successfully!\")\n",
    "    \n",
    "    # Example with French tokenizer\n",
    "    fr_text = \"Ceci est une phrase d'exemple.\"\n",
    "    fr_tokens = fr_tok.tokenize(fr_text)\n",
    "    print(f\"French text: {fr_text}\")\n",
    "    print(f\"Tokens: {fr_tokens}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load French models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The wikilangs package provides easy access to pre-trained language models from Wikipedia data.\n",
    "You can use these models for various NLP tasks including tokenization, text scoring, generation, and vocabulary lookup.\n",
    "\n",
    "For more information, check out the [documentation](https://github.com/omarkamali/wikilangs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
